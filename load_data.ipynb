{
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "lastEditStatus": {
   "notebookId": "kjbtcduyz6k6ruidxt4e",
   "authorId": "5744486210470",
   "authorName": "CCARRERO",
   "authorEmail": "carlos.carrero@snowflake.com",
   "sessionId": "bc0bcdf3-28a5-4c4c-af6c-fb253adf9365",
   "lastEditTime": 1736848799848
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1",
    "resultHeight": 41
   },
   "source": [
    "In this file, I use Snowpark to load the full insurance dataset csv(1M rows) into a snowflake table "
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "name": "cell2",
    "language": "python",
    "resultHeight": 127
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : COCONUT457\n",
      "Role                        : \"ACCOUNTADMIN\"\n",
      "Database                    : \"INSURANCE\"\n",
      "Schema                      : \"ML_PIPE\"\n",
      "Warehouse                   : \"COMPUTE_WH\"\n"
     ]
    }
   ],
   "source": "from snowflake.snowpark import Session\nimport json\nimport pandas as pd\n\n# Initiate session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "name": "cell3",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading insurance-data-for-machine-learning.zip to c:\\Users\\txsmi\\Documents\\Local Programming\\Snowflake\\Snowflake-ML-Pipeline\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/21.3M [00:00<?, ?B/s]\n",
      "  5%|▍         | 1.00M/21.3M [00:00<00:06, 3.20MB/s]\n",
      "  9%|▉         | 2.00M/21.3M [00:00<00:05, 3.87MB/s]\n",
      " 24%|██▎       | 5.00M/21.3M [00:00<00:01, 8.71MB/s]\n",
      " 28%|██▊       | 6.00M/21.3M [00:00<00:01, 8.59MB/s]\n",
      " 33%|███▎      | 7.00M/21.3M [00:01<00:01, 8.16MB/s]\n",
      " 42%|████▏     | 9.00M/21.3M [00:01<00:01, 9.06MB/s]\n",
      " 52%|█████▏    | 11.0M/21.3M [00:01<00:01, 9.85MB/s]\n",
      " 61%|██████    | 13.0M/21.3M [00:01<00:00, 10.2MB/s]\n",
      " 71%|███████   | 15.0M/21.3M [00:01<00:00, 10.7MB/s]\n",
      " 80%|███████▉  | 17.0M/21.3M [00:01<00:00, 10.4MB/s]\n",
      " 85%|████████▍ | 18.0M/21.3M [00:02<00:00, 10.2MB/s]\n",
      " 94%|█████████▍| 20.0M/21.3M [00:02<00:00, 10.3MB/s]\n",
      " 99%|█████████▉| 21.0M/21.3M [00:02<00:00, 10.1MB/s]\n",
      "100%|██████████| 21.3M/21.3M [00:02<00:00, 9.21MB/s]\n"
     ]
    }
   ],
   "source": "#!kaggle datasets download -d sridharstreaks/insurance-data-for-machine-learning --unzip",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell4",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Load full 1M dataset into dataframe\n",
    "insurance_df = pd.read_csv('insurance_dataset.csv')"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell5",
    "resultHeight": 67,
    "collapsed": false
   },
   "source": [
    "Use the write_pandas() method to write the first 10k rows into the 'SOURCE_OF_TRUTH' table created with the SQL commands in the SQL file. The method \"returns a Snowpark DataFrame object referring to the table where the pandas DataFrame was written to.\" (Snowpark Documentation)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "name": "cell6",
    "language": "python",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Capitalize column names\ninsurance_df.columns = insurance_df.columns.str.upper()\n\n# Rearrange columns to fit target schema\ncols = insurance_df.columns.tolist()\ncols = cols[:3] + cols[-1:] + cols[3:-1]\n\ninsurance_df = insurance_df[cols]\n\nsource_of_truth_df = session.write_pandas(insurance_df[:10000], table_name='SOURCE_OF_TRUTH',auto_create_table=True)",
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "The code below writes the remaining 990k to the INCOMING_DATA_SOURCE table (created automatically) to simulate data being streamed in"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "name": "cell8",
    "language": "python",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\txsmi\\miniconda3\\envs\\snowpark-env\\Lib\\site-packages\\snowflake\\snowpark\\session.py:2126: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.range.RangeIndex'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, nchunks, nrows, ci_output = write_pandas(\n"
     ]
    }
   ],
   "source": "incoming_data_source_df = session.write_pandas(insurance_df[10000:], table_name='INCOMING_DATA_SOURCE',auto_create_table=True)",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "id": "b1c0b1ee-b3c3-4c4f-9f83-4cc55357c024",
   "metadata": {
    "language": "sql",
    "name": "cell9",
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "create or replace stage ML_PIPE_STAGE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37416c15-de99-4cda-be31-d9fc1e6e7d02",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark\nfrom snowflake.snowpark.dataframe import col as column\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.functions import sproc\nimport snowflake.snowpark.types as T\n\nimport json\nimport pandas as pd\nimport numpy as np\n\n# Snowpark ML\nfrom snowflake.ml._internal.utils import identifier\nfrom snowflake.ml.registry import registry\n\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nimport snowflake.ml.modeling.preprocessing as snowmlpp\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error, mean_squared_error\n\n\n@sproc(name='predict_write_to_gold', stage_location='@ML_PIPE_STAGE', is_permanent=True, replace=True,packages=[\"snowflake-snowpark-python\",'snowflake-ml-python', 'xgboost','pandas'])\ndef predict_write_to_gold(session: Session) -> str:\n        try:    \n                df = session.table('STREAM_ON_LANDING').filter(column('METADATA$ACTION') == 'INSERT')\n\n        except Exception as e:\n                return (f'Error with reading from stream: {e}')\n\n        # Standardize values\n        try:\n                # Define Snowflake categorical types and determine which columns to OHE\n                categorical_types = [T.StringType]\n                cols_to_ohe = [col.name for col in df.schema.fields if (type(col.datatype) in categorical_types)]\n                ohe_cols_output = [col + '_OHE' for col in cols_to_ohe]\n\n                def fix_values(columnn):\n                        return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n                \n                for col in cols_to_ohe:\n                        df = df.na.fill('NONE', subset=col)\n                        df = df.withColumn(col, fix_values(col))\n\n        except Exception as e:\n                return (f'Error standardizing values {e}')\n\n        # Create model registry and load the default pipeline \n        try:\n                model_registry = registry.Registry(session=session, database_name=session.get_current_database(), schema_name='ML_PIPE')\n                model_version = model_registry.get_model('INSURANCE_CHARGES_PREDICTION').default\n\n        except Exception as e:\n                return (f'Error with creating model registry object: {e}')\n\n\n        # Run the pipeline\n        try:\n                results = model_version.run(df,function_name = 'predict')\n\n        except Exception as e:\n                return (f'Error with running model: {e}')\n\n        # Load the results into the gold table\n        try:\n                count = results.count()\n\n                cols_to_update = {col: results[col] for col in session.table('INSURANCE_GOLD').columns if 'METADATA_UPDATED_AT' not in col}\n                metadata_col_to_update = {'METADATA_UPDATED_AT': F.current_timestamp()}\n                updates = {**cols_to_update, **metadata_col_to_update}\n                target = session.table('INSURANCE_GOLD')\n                merge_results = target.merge(results,target['METADATA$ROW_ID'] == results['METADATA$ROW_ID'], \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n                \n\n                return (f'{merge_results.rows_inserted} record(s) inserted, {merge_results.rows_updated} record(s) updated in the INSURANCE_GOLD table')\n\n        except Exception as e:\n                return (f'Error with writing results to gold table: {e}')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b796f7ce-7767-4523-a821-2ef0e6a459fe",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Before running, make sure @ML_PIPE_STAGE exists (SQL File)\n# Create the sproc that creates and fits the pipeline based on the table passed\n@sproc(name='train_save_ins_model', stage_location='@ML_PIPE_STAGE', is_permanent=True, replace=True, packages=[\"snowflake-snowpark-python\",'snowflake-ml-python', 'xgboost','pandas'])\ndef train_save_ins_model(session: Session, source_of_truth: str, major_version: bool = True) -> str:\n\n    # Access the data from the source of truth table\n    try:\n        df = session.table(source_of_truth).limit(1000)\n\n    except Exception as e:\n        return (f'Error with getting table data: {e}')\n\n    # Define label and feature columns\n    LABEL_COLUMNS = ['CHARGES']\n    FEATURE_COLUMN_NAMES = [i for i in df.schema.names if i not in LABEL_COLUMNS]\n    OUTPUT_COLUMNS = ['PREDICTED_CHARGES']\n\n    # Define Snowflake numeric types (possibly for scaling, ordinal encoding)\n    # numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n    # numeric_columns = [col.name for col in df.schema.fields if (type(col.datatype) in numeric_types) and (col.name in FEATURE_COLUMN_NAMES)]\n\n    # Define Snowflake categorical types and determine which columns to OHE\n    categorical_types = [T.StringType]\n    cols_to_ohe = [col.name for col in df.schema.fields if (type(col.datatype) in categorical_types)]\n    ohe_cols_output = [col + '_OHE' for col in cols_to_ohe]\n\n\n    # Standardize the values in the rows by removing spaces, capitalizing\n    def fix_values(columnn):\n            return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\n    try:\n        for col in cols_to_ohe:\n                df = df.na.fill('NONE', subset=col)\n                df = df.withColumn(col, fix_values(col))\n\n    except Exception as e:\n        return (f'Error with standardizing values: {e}')\n\n    # Define the pipeline\n    try:\n        pipe = Pipeline(\n            steps=[\n                #('imputer', SimpleImputer(input_cols=all_cols)),\n                #('mms', snowmlpp.MinMaxScaler(input_cols=cols_to_scale, output_cols=scale_cols_output)),\n                ('ohe', snowmlpp.OneHotEncoder(input_cols=cols_to_ohe, output_cols=ohe_cols_output, drop_input_cols=True)),\n                ('grid_search_reg', GridSearchCV(estimator=XGBRegressor(),\n                                                    param_grid={ \"n_estimators\":[50], # 25\n                                                                \"learning_rate\":[0.4], # .5\n                                                                },\n                                                    n_jobs = -1,\n                                                    scoring=\"neg_mean_absolute_percentage_error\",\n                                                    input_cols=FEATURE_COLUMN_NAMES.append(ohe_cols_output),\n                                                    label_cols=LABEL_COLUMNS,\n                                                    output_cols=OUTPUT_COLUMNS\n                                                    )\n                )\n            ]      \n        )\n\n    except Exception as e:\n        return (f'Error with defining the pipeline: {e}')\n\n\n    # Split the data into training and testing\n    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n\n\n    # Fit the pipeline\n    try:\n        pipe.fit(train_df)\n\n    except Exception as e:\n        return (f'Error with fitting pipeline: {e}')\n\n\n    # Predict with the pipeline\n    try:\n        results = pipe.predict(test_df)\n\n    except Exception as e:\n        return (f'Error with predicting with pipeline: {e}')\n\n\n    # Use Snowpark ML metrics to calculate MAPE and MSE\n\n    # Calculate MAPE\n    mape = mean_absolute_percentage_error(df=results, y_true_col_names=LABEL_COLUMNS, y_pred_col_names=OUTPUT_COLUMNS)\n\n    # Calculate MSE\n    mse = mean_squared_error(df=results, y_true_col_names=LABEL_COLUMNS, y_pred_col_names=OUTPUT_COLUMNS)\n\n    def set_model_version(registry_object,model_name, major_version=True):\n        # See what we've logged so far, dynamically set the model version\n        import numpy as np\n        import json\n        \n        model_list = registry_object.show_models()\n        \n        if len(model_list) == 0:\n            return 'V1'\n        \n        model_list_filter = model_list[model_list['name'] ==  model_name]\n\n        if len(model_list_filter) == 0:\n            return 'V1'\n\n        version_list_string = model_list_filter['versions'].iloc[0]\n        version_list = json.loads(version_list_string)\n        version_numbers = [float(s.replace('V', '')) for s in version_list]\n        model_last_version = max(version_numbers)\n        \n        \n        if np.isnan(model_last_version) == True:\n            model_new_version = 'V1'\n\n        elif np.isnan(model_last_version) == False and major_version == True:\n            model_new_version = round(model_last_version + 1,2)\n            model_new_version = 'V' + str(model_new_version)\n            \n        else:\n            model_new_version = round(model_last_version + .1,2)\n            model_new_version = 'V' + str(model_new_version)\n            \n        return model_new_version # This is the version we will use when we log the new model.\n\n    # Create model regisry object\n    try:\n        model_registry = registry.Registry(session=session)\n\n    except Exception as e:\n        return (f'Error with creating model registry object: {e}')\n    \n    # Save model to registry\n    try:\n        LABEL_COLUMNS = ['CHARGES']\n        FEATURE_COLUMN_NAMES = [i for i in df.schema.names if i not in LABEL_COLUMNS]\n        X = train_df.select(FEATURE_COLUMN_NAMES).limit(100)\n\n        model_name = 'INSURANCE_CHARGES_PREDICTION'\n        version_name = set_model_version(model_registry, model_name, major_version=major_version)\n        model_version = model_registry.log_model(\n            model = pipe, \n            model_name = model_name, \n            version_name= f'\"{version_name}\"',\n            sample_input_data=X,\n            conda_dependencies=['snowflake-snowpark-python','snowflake-ml-python','scikit-learn', 'xgboost']\n            )\n\n        model_version.set_metric(metric_name='mean_abs_pct_err', value=mape)\n        model_version.set_metric(metric_name='mean_sq_err', value=mse)\n    \n    except Exception as e:\n        return (f'Error with saving model to registry: {e}')\n    \n    try:\n        session.sql(f'alter model INSURANCE_CHARGES_PREDICTION set default_version = \"{version_name}\";')\n    \n    except Exception as e:\n        return (f'Error with setting default version: {e}')\n\n    return f'Model {model_name} has been logged with version {version_name} and has a MAPE of {mape} and MSE of {mse}'\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c289ccbe-90f8-4e31-a885-efea9e11c56d",
   "metadata": {
    "language": "sql",
    "name": "cell12",
    "codeCollapsed": false,
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "\n-- Create the task that calls the training sproc\nCREATE or REPLACE TASK TRAIN_SAVE_TASK\n  WAREHOUSE = WH\n  SCHEDULE = '5 MINUTE' --\n  AS\n    CALL train_save_ins_model('SOURCE_OF_TRUTH',FALSE);\n\n-- Tasks are created in a suspended state. Resume it\nALTER TASK TRAIN_SAVE_TASK RESUME;\n\n-- Execute immediately so that you have a trained model in registry\nEXECUTE TASK TRAIN_SAVE_TASK;\n\n-- Create the landing table (where streamed-in records could land)\nCREATE or REPLACE TABLE LANDING_TABLE (\n\tAGE NUMBER(38,0),\n\tGENDER VARCHAR(16777216),\n\tBMI FLOAT,\n\tCHARGES FLOAT,\n\tCHILDREN NUMBER(38,0),\n\tSMOKER VARCHAR(16777216),\n\tREGION VARCHAR(16777216),\n\tMEDICAL_HISTORY VARCHAR(16777216),\n\tFAMILY_MEDICAL_HISTORY VARCHAR(16777216),\n\tEXERCISE_FREQUENCY VARCHAR(16777216),\n\tOCCUPATION VARCHAR(16777216),\n\tCOVERAGE_LEVEL VARCHAR(16777216)\n);\n\n-- Create the stream on the landing table\nCREATE OR REPLACE STREAM STREAM_ON_LANDING ON TABLE LANDING_TABLE;\n\n-- Create a gold table for the records and their predictions to land\nCREATE OR REPLACE TABLE INSURANCE_GOLD(\n    AGE NUMBER(38,0),\n\tGENDER VARCHAR(16777216),\n\tBMI FLOAT,\n\tCHILDREN NUMBER(38,0),\n\tSMOKER VARCHAR(16777216),\n\tREGION VARCHAR(16777216),\n\tMEDICAL_HISTORY VARCHAR(16777216),\n\tFAMILY_MEDICAL_HISTORY VARCHAR(16777216),\n\tEXERCISE_FREQUENCY VARCHAR(16777216),\n\tOCCUPATION VARCHAR(16777216),\n\tCOVERAGE_LEVEL VARCHAR(16777216),\n    METADATA$ROW_ID VARCHAR(16777216),\n    METADATA$ISUPDATE BOOLEAN,\n    METADATA$ACTION VARCHAR(16777216),\n    METADATA_UPDATED_AT DATE,\n    CHARGES FLOAT,\n    PREDICTED_CHARGES FLOAT\n);\n\n-- Insert records into the landing table to simulate streamed data\nINSERT INTO LANDING_TABLE(\n    AGE ,\n\tGENDER,\n\tBMI,\n\tCHARGES ,\n\tCHILDREN,\n\tSMOKER,\n\tREGION,\n\tMEDICAL_HISTORY ,\n\tFAMILY_MEDICAL_HISTORY,\n\tEXERCISE_FREQUENCY ,\n\tOCCUPATION ,\n\tCOVERAGE_LEVEL\n) SELECT \n    AGE,\n\tGENDER,\n\tBMI,\n\tCHARGES ,\n\tCHILDREN,\n\tSMOKER,\n\tREGION,\n\tMEDICAL_HISTORY ,\n\tFAMILY_MEDICAL_HISTORY,\n\tEXERCISE_FREQUENCY ,\n\tOCCUPATION ,\n\tCOVERAGE_LEVEL\nFROM INCOMING_DATA_SOURCE\nLIMIT 100; -- Change this number to test prediction speed at different quantities\n\n-- View the inserted records in the stream, along with the added metadata columns\nSELECT * FROM STREAM_ON_LANDING;\n\n-- Run the predict_and_write.py file here. This will create the prediction/write to gold sproc\n\n-- Call the prediction SPROC to see it work on the data you loaded into the stream.\nCALL PREDICT_WRITE_TO_GOLD();\n\n-- Testing the capacity to update records that already exist in the gold table\nupdate landing_table set coverage_level = 'STANDARD'\nwhere age = 41;\n\n-- View the stream after updating\nSELECT * FROM STREAM_ON_LANDING;\n\n-- Call the prediction SPROC again to see how it handles updates\nCALL PREDICT_WRITE_TO_GOLD();\n\n-- Create the predict and write task\nCREATE or REPLACE TASK PREDICT_WRITE_TASK\n  WAREHOUSE = WH\n  SCHEDULE = '1 MINUTE'\n  WHEN\n    SYSTEM$STREAM_HAS_DATA('STREAM_ON_LANDING')\n  AS\n    CALL PREDICT_WRITE_TO_GOLD();\n\n-- Again, tasks are created in a suspended state. Resume it\nALTER TASK PREDICT_WRITE_TASK RESUME;\n\n-- Clean up\n--ALTER TASK PREDICT_WRITE_TASK SUSPEND;\n--ALTER TASK TRAIN_SAVE_TASK SUSPEND;\n--DROP DATABASE INSURANCE;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36b10e1c-4166-45f0-8e28-53fd1c82c372",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "collapsed": false,
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CALL train_save_ins_model('SOURCE_OF_TRUTH',FALSE);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8e75225-e24d-4eb3-8cb4-90adfe5243db",
   "metadata": {
    "language": "sql",
    "name": "cell14",
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "EXECUTE TASK TRAIN_SAVE_TASK;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b86cf552-19c5-4170-aef7-ddefbb8fb75b",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf776163-3ef4-431c-bb09-62a2380e5582",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}